{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "9_g20zmfEXhj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MieXrWGPMNRZ",
        "outputId": "195dd9a9-d4e3-49d6-8536-48a31ff52f45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n"
      ],
      "metadata": {
        "id": "0wQRwEDAKdf3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZZF0K3rMEhvR"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/drive/MyDrive/train.zip\", 'r') as zObject:\n",
        "\n",
        "    # Extracting specific file in the zip\n",
        "    # into a specific location.\n",
        "    zObject.extractall(path=\"/content/unzip\")\n",
        "zObject.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "clnZzWwfMEep"
      },
      "outputs": [],
      "source": [
        "# transform = transforms.Compose(\n",
        "#     [transforms.RandomHorizontalFlip(0.5),\n",
        "#      transforms.RandomVerticalFlip(0.5),\n",
        "#      transforms.RandomRotation(10),\n",
        "#      transforms.ToTensor(),\n",
        "#      transforms.RandomErasing(0.1),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# batch_size=64\n",
        "\n",
        "\n",
        "# data_set = datasets.ImageFolder(\"/content/unzip/train/\", transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "     [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "batch_size=64\n",
        "\n",
        "\n",
        "data_set = datasets.ImageFolder(\"/content/unzip/train/\", transform = transform)"
      ],
      "metadata": {
        "id": "l0K88-M0RjfR"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crEHPotlQvfW",
        "outputId": "fcf5a5eb-b73d-4a93-af03-1e0183b1927b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[-1.0000, -1.0000, -1.0000,  ..., -0.2471, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -0.3647, -1.0000, -1.0000],\n",
              "          [-0.0353,  0.0039,  0.0431,  ..., -0.3490, -1.0000, -1.0000],\n",
              "          ...,\n",
              "          [-1.0000, -1.0000, -0.2706,  ..., -0.7255, -0.6941, -0.7098],\n",
              "          [-1.0000, -1.0000, -0.3020,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -0.3490,  ..., -1.0000, -1.0000, -1.0000]],\n",
              " \n",
              "         [[-1.0000, -1.0000, -1.0000,  ..., -0.1373, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -0.2549, -1.0000, -1.0000],\n",
              "          [ 0.0745,  0.1137,  0.1294,  ..., -0.2392, -1.0000, -1.0000],\n",
              "          ...,\n",
              "          [-1.0000, -1.0000, -0.1451,  ..., -0.6078, -0.5765, -0.5922],\n",
              "          [-1.0000, -1.0000, -0.1765,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -0.2235,  ..., -1.0000, -1.0000, -1.0000]],\n",
              " \n",
              "         [[-1.0000, -1.0000, -1.0000,  ..., -0.1137, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -1.0000,  ..., -0.2314, -1.0000, -1.0000],\n",
              "          [ 0.0824,  0.1216,  0.1451,  ..., -0.2157, -1.0000, -1.0000],\n",
              "          ...,\n",
              "          [-1.0000, -1.0000, -0.1451,  ..., -0.5843, -0.5529, -0.5686],\n",
              "          [-1.0000, -1.0000, -0.1765,  ..., -1.0000, -1.0000, -1.0000],\n",
              "          [-1.0000, -1.0000, -0.2235,  ..., -1.0000, -1.0000, -1.0000]]]),\n",
              " 49)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "DsWLLwHfE0AU"
      },
      "outputs": [],
      "source": [
        "dataset_size = len(data_set)\n",
        "\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "\n",
        "train_set, val_set = random_split(data_set, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform2 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomErasing(0.1),\n",
        "])\n",
        "\n",
        "train_set.transform = transform2"
      ],
      "metadata": {
        "id": "37vGGbbAYYDX"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=8)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "a2mHqWTkSF0v"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Lz6iQHBoMEeq"
      },
      "outputs": [],
      "source": [
        "#Więcej warstw konwolucyjnych, mniejszy kernel\n",
        "\n",
        "class Net_More(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## Warstwa konwolucyjna\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=1, padding=0)\n",
        "        ## Warstwa max pooling\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(64, 256, 4)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = nn.Conv2d(256,512,3)\n",
        "        self.pool3 = nn.MaxPool2d(2,2,1)\n",
        "        self.conv4 = nn.Conv2d(512,64,3)\n",
        "        self.fc1 = nn.Linear(1024, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        # print(x.size())\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        # print(x.size())\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        # print(x.size())\n",
        "        x = F.relu(self.conv4(x))\n",
        "        # print(x.size())\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        # print(x.size())\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mniej warstw konwolucyjnych, większy kernel, więcej paddingu\n",
        "\n",
        "class Net_Less(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## Warstwa konwolucyjna\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=5, stride=1, padding=1)\n",
        "        ## Warstwa max pooling\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(128, 512, 5,1)\n",
        "        self.pool2 = nn.MaxPool2d(2,2,1)\n",
        "        self.conv3 = nn.Conv2d(512,128,4)\n",
        "        self.pool3 = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(3200, 500)\n",
        "        self.fc2 = nn.Linear(500, 100)\n",
        "        self.fc3 = nn.Linear(100, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        # print(x.size())\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        # print(x.size())\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        # print(x.size())\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        # print(x.size())\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sMEcan8-Nx2-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dnT6LEBtMEer"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net_Less().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "lQ2RfrsBMEes",
        "outputId": "4ad10f2b-e161-40d7-af31-22e5a2fd126b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] loss: 1.819\n",
            "[2/50] loss: 1.594\n",
            "[3/50] loss: 1.481\n",
            "[4/50] loss: 1.412\n",
            "[5/50] loss: 1.362\n",
            "[6/50] loss: 1.323\n",
            "[7/50] loss: 1.292\n",
            "[8/50] loss: 1.266\n",
            "[9/50] loss: 1.243\n",
            "[10/50] loss: 1.223\n",
            "[11/50] loss: 1.208\n",
            "[12/50] loss: 1.196\n",
            "[13/50] loss: 1.181\n",
            "[14/50] loss: 1.165\n",
            "[15/50] loss: 1.154\n",
            "[16/50] loss: 1.144\n",
            "[17/50] loss: 1.132\n",
            "[18/50] loss: 1.121\n",
            "[19/50] loss: 1.118\n",
            "[20/50] loss: 1.106\n",
            "[21/50] loss: 1.094\n",
            "[22/50] loss: 1.090\n",
            "[23/50] loss: 1.084\n",
            "[24/50] loss: 1.075\n",
            "[25/50] loss: 1.068\n",
            "[26/50] loss: 1.064\n",
            "[27/50] loss: 1.058\n",
            "[28/50] loss: 1.052\n",
            "[29/50] loss: 1.049\n",
            "[30/50] loss: 1.042\n",
            "[31/50] loss: 1.041\n",
            "[32/50] loss: 1.030\n",
            "[33/50] loss: 1.026\n",
            "[34/50] loss: 1.021\n",
            "[35/50] loss: 1.018\n",
            "[36/50] loss: 1.012\n",
            "[37/50] loss: 1.007\n",
            "[38/50] loss: 1.006\n",
            "[39/50] loss: 1.000\n",
            "[40/50] loss: 0.997\n",
            "[41/50] loss: 0.997\n",
            "[42/50] loss: 0.993\n",
            "[43/50] loss: 0.988\n",
            "[44/50] loss: 0.982\n",
            "[45/50] loss: 0.982\n",
            "[46/50] loss: 0.974\n",
            "[47/50] loss: 0.973\n",
            "[48/50] loss: 0.968\n",
            "[49/50] loss: 0.968\n",
            "[50/50] loss: 0.958\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print('[%d/%d] loss: %.3f' %\n",
        "          (epoch+1 ,  num_epochs, running_loss / 2000))\n",
        "    running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in valloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images).cpu()\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfxZaF_BKS2L",
        "outputId": "0784c183-f67d-4ced-e767-7786636fc632"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 53 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}