{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "aF7NhtuRuj3m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQk7qP3Wszsx",
        "outputId": "ff8b059e-9db4-44a5-abba-55d43b60dcbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlL-F0T8ujBn",
        "outputId": "509df563-8104-4f0c-8c26-f8481d9ff25b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/train.pkl', 'rb') as f:\n",
        "    data_set = pickle.load(f)"
      ],
      "metadata": {
        "id": "HRKMm6B2uwFd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dz5vfxHvMmg",
        "outputId": "93ec3e95-11cc-4033-a414-f9dfc7a80f6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "targets = []\n",
        "for i in data_set:\n",
        "  data.append(i[0])\n",
        "  targets.append(i[1])"
      ],
      "metadata": {
        "id": "viCp1NWgEI53"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_array = np.array(targets)\n",
        "targets_array_reshaped = targets_array.reshape(-1, 1)\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "ohe.fit(targets_array_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "x-GkoyTUrksk",
        "outputId": "d760abc2-f79c-46e7-e957-78bdfaebc90f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets_ohe = ohe.transform(targets_array_reshaped)\n",
        "print(targets_ohe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52b_-RYisjYn",
        "outputId": "4e57a36f-b9f3-4bd7-b8b5-1d790d943b53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[-1])\n",
        "print(targets[-1])\n",
        "print(targets_ohe[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6eNqA5MIIcl",
        "outputId": "8f783abf-88d5-4223-df23-d7280606a5b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112. 112.   0.  64.   0. 112.  34. 144. 146. 112. 112. 179. 112. 112.\n",
            "  73.   3.  12. 112. 146.  60.  45. 112.  73.  88. 117.   8. 159. 145.\n",
            " 159. 112.  13.  80.  92. 159. 190. 112. 185. 190.   8.  73. 125.  77.\n",
            "  12.   0. 117. 190. 185. 112. 125. 125.  45.  41.  92.  28.  92.  12.\n",
            " 159.   0.  47.  12.  12.  88.  88.   0.  12. 112. 159.   8. 159.   0.\n",
            "  12. 112. 125.  23.  47.  41. 159. 112.  45.  12.   8. 145.  47.  33.\n",
            "  33.  33. 125.  73.   8.  13.  92.  88. 159. 120. 124. 185. 185.  77.\n",
            " 125.  88.  13. 120. 120. 112. 112. 112. 159. 172.  12. 117.  47.  88.\n",
            "  88.  36.  92.  33. 125.  88.  88.  88.  12. 112. 159.  88. 125. 119.\n",
            "  47.  88.  92. 159. 185. 112.  77.  33. 125.  65. 125.  33. 125.  88.\n",
            "  13. 120. 124. 112.  12.   0.   0.   0.  12. 112. 159. 145.   5.   0.\n",
            "  88. 112.  12.  47.  88.   0.  12. 112. 159. 145.   5.   0.  12. 112.\n",
            " 125. 125.  28. 159.  28.  38.  12. 117.  47. 114. 153.  88. 159.   8.\n",
            "   8. 112.  77.  38.  12.   0. 117. 190. 185. 112.  77.  33.  41. 145.\n",
            " 145. 106. 124. 112.  74.  41.  92.  88.  12. 112.  74.  41.  13. 117.\n",
            "  92.  80. 125. 185. 190. 114. 112. 112. 159.  88.  13.   8.  12. 117.\n",
            "  12. 125. 124. 106. 159. 112. 112. 112. 112. 112.  12.   0. 117. 190.\n",
            " 185. 112. 112. 112. 112. 112.]\n",
            "4\n",
            "[0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "labels = [label for _, label in data_set]\n",
        "\n",
        "class_counts = Counter(labels)\n",
        "classes = []\n",
        "print(\"Liczba klas:\", len(class_counts))\n",
        "for class_label, count in class_counts.items():\n",
        "    classes.append(count)\n",
        "classes = torch.tensor(classes)\n",
        "print(classes)\n",
        "class_weights = 1.0 / classes\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "6xQID-J53J_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c393ea5-7b74-4772-a255-7a06e55a65d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba klas: 5\n",
            "tensor([1630,  478,  154,  441,  236])\n",
            "tensor([0.0391, 0.1332, 0.4135, 0.1444, 0.2698])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KO0y2QIaXYfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariableLenDataset(Dataset):\n",
        "    def __init__(self, in_data, target):\n",
        "        self.data = [(x, y) for x, y in zip(in_data, target)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        in_data, target = self.data[idx]\n",
        "        return in_data, target"
      ],
      "metadata": {
        "id": "JTnWdmaG49BT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices = np.random.rand(len(data_set))>0.3"
      ],
      "metadata": {
        "id": "5JUCzIp8O5NX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices"
      ],
      "metadata": {
        "id": "Bai0Mly3O9Wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c06cd24-cedb-417d-cbd5-d754479e7639"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, ...,  True,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = []\n",
        "targets_train = []\n",
        "data_test = []\n",
        "targets_test = []\n",
        "for i in range(len(data_set)):\n",
        "  if train_indices[i] == True:\n",
        "    data_train.append(data_set[i][0])\n",
        "    targets_train.append(targets_ohe[i])\n",
        "  else:\n",
        "    data_test.append(data_set[i][0])\n",
        "    targets_test.append(targets_ohe[i])"
      ],
      "metadata": {
        "id": "3gDvCohVPFh-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_size = len(data_set)\n",
        "\n",
        "# train_size = int(0.8 * dataset_size)\n",
        "# test_size = dataset_size - train_size\n",
        "\n",
        "\n",
        "# train_set = VariableLenDataset(data[:train_indices], targets[:train_indices])\n",
        "# test_set = VariableLenDataset(data[train_indices:], targets[train_indices:])"
      ],
      "metadata": {
        "id": "qGbQf80xGZbU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = VariableLenDataset(data_train, targets_train)\n",
        "test_set = VariableLenDataset(data_test, targets_test)"
      ],
      "metadata": {
        "id": "05cbXnJZMV_M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set[2])"
      ],
      "metadata": {
        "id": "_gSWK_fJK6EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c559213c-b4e4-437f-e954-ad5c861763fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 66., 100., 148., 148., 146.,  64., 146., 148.,  82.,   0.,  82.,\n",
            "       100.,  34., 132., 180.,  65.,  80.,  81., 131.,  52.,  34.,  52.,\n",
            "        64.,  52.,   3.,  66., 147.,  20.,   4., 132., 132., 100., 111.,\n",
            "        74., 110.,  60.,  92.,  65., 100., 189.,  44.,   8.,   5.,  76.,\n",
            "        31., 159.,   5., 124.,   4.,  12.,  51., 157.,  57.,  31., 183.,\n",
            "        57.,  65.,  92.,  69., 124., 122.,  79., 110.,  76.,  12.,  12.,\n",
            "        12.,  12.,   8., 159.,  12., 156., 100., 111.,  52., 121.,  36.,\n",
            "        47.,  41.,  41.,   8., 172.,  38.,  12.,  78.,  12.,  88.,  47.,\n",
            "       119.,  20.,  47.,  12., 159.,  20.,  20.,  76.,  60., 110., 132.,\n",
            "       185., 120.,  45., 110., 110.,  72., 124., 178.,  73.,   8.,  78.,\n",
            "        12.,  42., 173.,  12., 150.,  84.,  66., 152.,  69.,   8.,  41.,\n",
            "       159.,   5.,  78.,  44., 180.,  71.,  92., 152.,   6.,  12.,  45.,\n",
            "        92., 125.,   8., 156.,  12.,  12., 146.,   8., 185.,   0.,  88.,\n",
            "        92.,  90.,  12.,  12.,  12.,  78.,  92.,   3.,  45., 111., 111.,\n",
            "        71., 137.,  69.,  12.,  88.,  12.,  10.,   8., 172., 156., 152.,\n",
            "       100.,  12.,  12.,  77.,  52., 156., 148.,  12.,  12., 132.,   6.,\n",
            "         6., 185.,  47., 121.,  20.,  90.,  64., 111.,  92., 111.,  77.,\n",
            "        76.,  69., 100.,   8.,  76.,  76.,  69., 132.,   6.,   6.,   8.,\n",
            "        20.,  73.,  77., 111.,   6.,   8.,  13.,  12., 178.,   5.,  88.,\n",
            "       159., 121.,  58.,  44.,  47., 100., 156.,  12.,  41.,  88.,  73.,\n",
            "        20., 125.,  12., 124.,   5.,   0.,   5.,  47.,  12.,  12.,  12.,\n",
            "        12.,   8., 159., 185., 124., 100., 111.,  52., 121.,  36.,  47.,\n",
            "        41.,  41., 180.,   8.,  66.,   8., 146., 180.,  88.,  47., 119.,\n",
            "        20.,  73., 172., 158.,  20.,  20.,  20.,  78.,  69.,  12.,  12.,\n",
            "        47.,  47.,  47.,  30.,   8.,   6.,  45.,  12., 111., 111.,  38.,\n",
            "        12.,  12.,  12., 148.,   6.,   6., 185.,  47., 121.,  20.,  78.,\n",
            "        64., 111.,  77., 111.,  77.,  76., 141., 100.,   8.,  76.,  76.,\n",
            "         6., 132., 100., 111.,  74., 110.,  60., 159.,  79.,   5., 189.,\n",
            "       110.,  12.,   5.,  76.,  10.,  92.,   5., 124.,   4.,  12.,  51.,\n",
            "       157.,  57.,  31., 183.,  57.,  65.,  92.,  69., 124., 122.,  79.,\n",
            "       110.,  76.,  10.,  12.,  12.,  12.,   8., 159.,  12., 156., 100.,\n",
            "       111.,  52., 121.,  36.,  47.,  41.,  41.,   8., 172.,  38.,  12.,\n",
            "        78.,  12.,  88.,  47., 119.,  20.,  47.,  12.,  12.,  78.,  63.,\n",
            "        76.,  60., 110., 159., 132., 124.,  12.,  15., 110., 110.,  72.,\n",
            "       124., 178.,  73.,   8.,  78.,  12.,  12.,  30.,  12., 150.,  84.,\n",
            "        66., 152.,  69.,   8.,  41., 159.,   5.,  78.,  44., 180.,  71.,\n",
            "        92., 156., 132.,  79.,  45.,  92., 125.,   8., 156.,  12.,  12.,\n",
            "       146.,  12., 185.,   0.,  92., 108.,  90.,  47.,  12.,  12., 125.,\n",
            "        92.,   3.,  45., 111., 111.,  71., 137.,  69.,  12.,  47.,  12.,\n",
            "        10.,   8.,  41., 156., 152., 100.,  12.,  12.,  74.,  52., 156.,\n",
            "       148., 148.,  12.,  69.,   6.,   6., 185.,  47., 121.,  20.,  90.,\n",
            "        64., 111.,  92., 111.,  77.,  76., 141., 100.,   8.,  76.,  76.,\n",
            "        69., 132., 100.,   6.,   8.,  88.,  47., 141.,  77.,   6.,   8.,\n",
            "       152.,  12., 178.,   5.,  88., 159., 121.,  58.,  44.,  47., 100.,\n",
            "       156.,  12.,  41.,  88.,  73.,  20., 125.,  12., 124.,   5.,   0.,\n",
            "         5.,  47.,  12.,  12.,  12.,  12.,  12., 159., 185., 124.,   6.,\n",
            "       111.,  52., 121.,  36.,  47.,  41.,  41., 180.,   8.,  66.,   8.,\n",
            "       146., 180.,  88.,  47., 119.,  20.,  73.,  12., 158.,  20.,  20.,\n",
            "        20.,  78.,  69.,  12.,  12.,  47.,  47.,  47., 125.,   8.,   6.,\n",
            "        12.,  12., 111., 111.,  38.,  12.,  12.,  12.,  82.,   6.,   6.,\n",
            "       185.,  47., 121.,  88.,  90.,  64., 111.,  92., 111.,  77.,  76.,\n",
            "       141., 100.,   8.,  76.,  76.,  69., 132.,   6.,   6.,   8.,  47.,\n",
            "        47.,  10.,  10.,  82.,   8., 148.,  38.,  50.,  50.,  34.,  34.,\n",
            "        34.,  34.,  34.,  34., 144., 144., 144., 144.,  -1.]), array([1., 0., 0., 0., 0.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad = -100\n",
        "\n",
        "# def pad_collate(batch, pad_value=pad):\n",
        "#     xx, yy = zip(*batch)\n",
        "#     xx = [torch.tensor(x) for x in xx]\n",
        "#     x_lens = [len(x) for x in xx]\n",
        "#     yy_pad = torch.tensor(yy)\n",
        "#     y_lens = [1] * len(yy)\n",
        "#     # print(xx)\n",
        "#     # print(yy)\n",
        "\n",
        "#     xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
        "#     # yy_pad = pad_sequence(yy, batch_first=True, padding_value=pad_value)\n",
        "\n",
        "#     return xx_pad, yy_pad, x_lens, y_lens\n",
        "\n",
        "def pad_collate(batch, pad_value=pad):\n",
        "    try:\n",
        "        xx, yy = zip(*batch)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error while unpacking batch: {e}\")\n",
        "        print(f\"Batch content: {batch}\")\n",
        "        raise\n",
        "\n",
        "    xx = [torch.tensor(x) for x in xx]\n",
        "    x_lens = [len(x) for x in xx]\n",
        "    yy_pad = torch.tensor(yy)\n",
        "    y_lens = [1] * len(yy)\n",
        "\n",
        "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
        "\n",
        "    return xx_pad, yy_pad, x_lens, y_lens"
      ],
      "metadata": {
        "id": "gT4DVyAfws-7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=256\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "xBcnjNFqIn6H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_size = len(data)\n",
        "\n",
        "# train_size = int(0.8 * dataset_size)\n",
        "# test_size = dataset_size - train_size\n",
        "\n",
        "# train_set, test_set = random_split(data, [train_size, test_size])\n",
        "\n",
        "# batch_size=4\n",
        "# trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "# testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "nE3dVfPevMd-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[0]\n"
      ],
      "metadata": {
        "id": "n4cE0wPFIndl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e8872a-2d88-4c80-bcc2-3af33473a4fe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ -1.,  -1.,  -1., ...,  78.,  40., 144.]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_set[0][0]))\n",
        "print(len(train_set[1][0]))"
      ],
      "metadata": {
        "id": "8iAC0VKIw776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce55772-f075-40bd-8d94-e93d87036eef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4756\n",
            "5322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R2Ulv0AxYvE",
        "outputId": "5f540f1a-52f9-4df1-b14e-6ca8d3f2cbb8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-76b5b91029d1>:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  yy_pad = torch.tensor(yy)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 144.,  144.,  144.,  ..., -100., -100., -100.],\n",
              "         [  -1.,   -1.,    0.,  ..., -100., -100., -100.],\n",
              "         [  12.,   12.,  157.,  ..., -100., -100., -100.],\n",
              "         ...,\n",
              "         [  -1.,   -1.,   -1.,  ..., -100., -100., -100.],\n",
              "         [ 144.,   69.,  152.,  ..., -100., -100., -100.],\n",
              "         [  -1.,  114.,  114.,  ..., -100., -100., -100.]], dtype=torch.float64),\n",
              " tensor([[0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 1.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 1., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0.]], dtype=torch.float64),\n",
              " [292,\n",
              "  416,\n",
              "  63,\n",
              "  176,\n",
              "  594,\n",
              "  2860,\n",
              "  636,\n",
              "  213,\n",
              "  282,\n",
              "  1176,\n",
              "  267,\n",
              "  1083,\n",
              "  75,\n",
              "  237,\n",
              "  354,\n",
              "  512,\n",
              "  450,\n",
              "  280,\n",
              "  180,\n",
              "  84,\n",
              "  332,\n",
              "  249,\n",
              "  648,\n",
              "  84,\n",
              "  32,\n",
              "  528,\n",
              "  2097,\n",
              "  64,\n",
              "  128,\n",
              "  64,\n",
              "  195,\n",
              "  180,\n",
              "  48,\n",
              "  20,\n",
              "  1292,\n",
              "  320,\n",
              "  500,\n",
              "  1440,\n",
              "  369,\n",
              "  256,\n",
              "  504,\n",
              "  64,\n",
              "  138,\n",
              "  279,\n",
              "  255,\n",
              "  231,\n",
              "  335,\n",
              "  228,\n",
              "  384,\n",
              "  528,\n",
              "  389,\n",
              "  140,\n",
              "  1094,\n",
              "  870,\n",
              "  140,\n",
              "  309,\n",
              "  266,\n",
              "  376,\n",
              "  360,\n",
              "  195,\n",
              "  152,\n",
              "  68,\n",
              "  108,\n",
              "  1029,\n",
              "  624,\n",
              "  531,\n",
              "  228,\n",
              "  614,\n",
              "  60,\n",
              "  432,\n",
              "  315,\n",
              "  297,\n",
              "  148,\n",
              "  52,\n",
              "  142,\n",
              "  146,\n",
              "  444,\n",
              "  108,\n",
              "  530,\n",
              "  60,\n",
              "  708,\n",
              "  60,\n",
              "  614,\n",
              "  75,\n",
              "  714,\n",
              "  236,\n",
              "  1120,\n",
              "  270,\n",
              "  379,\n",
              "  328,\n",
              "  200,\n",
              "  267,\n",
              "  208,\n",
              "  440,\n",
              "  168,\n",
              "  213,\n",
              "  224,\n",
              "  192,\n",
              "  344,\n",
              "  320,\n",
              "  1008,\n",
              "  212,\n",
              "  176,\n",
              "  556,\n",
              "  200,\n",
              "  417,\n",
              "  304,\n",
              "  902,\n",
              "  140,\n",
              "  291,\n",
              "  240,\n",
              "  748,\n",
              "  292,\n",
              "  330,\n",
              "  384,\n",
              "  81,\n",
              "  232,\n",
              "  200,\n",
              "  216,\n",
              "  312,\n",
              "  672,\n",
              "  2958,\n",
              "  68,\n",
              "  81,\n",
              "  159,\n",
              "  168,\n",
              "  485,\n",
              "  126,\n",
              "  333,\n",
              "  2303,\n",
              "  396,\n",
              "  1388,\n",
              "  306,\n",
              "  68,\n",
              "  512,\n",
              "  356,\n",
              "  418,\n",
              "  322,\n",
              "  96,\n",
              "  102,\n",
              "  268,\n",
              "  388,\n",
              "  354,\n",
              "  196,\n",
              "  513,\n",
              "  250,\n",
              "  248,\n",
              "  2268,\n",
              "  340,\n",
              "  228,\n",
              "  2096,\n",
              "  348,\n",
              "  92,\n",
              "  92,\n",
              "  132,\n",
              "  282,\n",
              "  580,\n",
              "  480,\n",
              "  92,\n",
              "  592,\n",
              "  135,\n",
              "  112,\n",
              "  75,\n",
              "  627,\n",
              "  768,\n",
              "  2450,\n",
              "  424,\n",
              "  2320,\n",
              "  1935,\n",
              "  2975,\n",
              "  1193,\n",
              "  1393,\n",
              "  345,\n",
              "  336,\n",
              "  132,\n",
              "  492,\n",
              "  52,\n",
              "  261,\n",
              "  48,\n",
              "  189,\n",
              "  260,\n",
              "  176,\n",
              "  308,\n",
              "  136,\n",
              "  338,\n",
              "  68,\n",
              "  440,\n",
              "  172,\n",
              "  522,\n",
              "  250,\n",
              "  148,\n",
              "  153,\n",
              "  405,\n",
              "  12,\n",
              "  144,\n",
              "  376,\n",
              "  336,\n",
              "  366,\n",
              "  288,\n",
              "  304,\n",
              "  735,\n",
              "  148,\n",
              "  326,\n",
              "  188,\n",
              "  216,\n",
              "  222,\n",
              "  222,\n",
              "  234,\n",
              "  520,\n",
              "  156,\n",
              "  64,\n",
              "  980,\n",
              "  76,\n",
              "  231,\n",
              "  72,\n",
              "  768,\n",
              "  112,\n",
              "  173,\n",
              "  387,\n",
              "  216,\n",
              "  497,\n",
              "  70,\n",
              "  116,\n",
              "  80,\n",
              "  1707,\n",
              "  228,\n",
              "  228,\n",
              "  360,\n",
              "  64,\n",
              "  184,\n",
              "  135,\n",
              "  306,\n",
              "  182,\n",
              "  416,\n",
              "  147,\n",
              "  910,\n",
              "  237,\n",
              "  483,\n",
              "  288,\n",
              "  400,\n",
              "  279,\n",
              "  225,\n",
              "  306,\n",
              "  340,\n",
              "  357,\n",
              "  198,\n",
              "  76,\n",
              "  588,\n",
              "  64,\n",
              "  104,\n",
              "  68,\n",
              "  889,\n",
              "  528,\n",
              "  2124,\n",
              "  108,\n",
              "  228],\n",
              " [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMRegressor(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, out_size, bidirectional = False):\n",
        "#         super().__init__()\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_size = hidden_size\n",
        "#         if bidirectional:\n",
        "#             self.bidirectional = 2\n",
        "#         else:\n",
        "#             self.bidirectional = 1\n",
        "#         self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "#         self.fc = nn.Linear(320, out_size)\n",
        "#         # self.fc = nn.Linear(hidden_size*90*self.bidirectional, out_size)\n",
        "\n",
        "#     def init_hidden(self, batch_size):\n",
        "#         hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         return hidden, state\n",
        "\n",
        "#     def forward(self, x, hidden):\n",
        "#         # x = torch.transpose(x,0,1)\n",
        "#         x = x.float()\n",
        "#         all_outputs, hidden = self.lstm(x, hidden)\n",
        "#         # all_outputs = torch.transpose(all_outputs,0,1)\n",
        "#         # out = torch.flatten(all_outputs,1)\n",
        "#         # output, _ = pad_packed_sequence(all_outputs, batch_first=True)\n",
        "#         # output = output[:, -1, :]\n",
        "\n",
        "#         h_n, _ = hidden\n",
        "#         h_n = torch.flatten(h_n, 1)  # Flatten the hidden state\n",
        "#         # print(h_n[0])\n",
        "#         x = self.fc(h_n)\n",
        "\n",
        "#         # hidden = torch.flatten(hidden,1)\n",
        "#         # print(hidden[0])\n",
        "#         # x = self.fc(hidden)\n",
        "#         return x, hidden\n",
        "\n",
        "# model = LSTMRegressor(1,5,2,5).to(device)\n",
        "# model"
      ],
      "metadata": {
        "id": "PcFIdl-fB0GW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMRegressor(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, out_size, bidirectional = False):\n",
        "#         super().__init__()\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_size = hidden_size\n",
        "#         if bidirectional:\n",
        "#             self.bidirectional = 2\n",
        "#         else:\n",
        "#             self.bidirectional = 1\n",
        "#         self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "#         self.fc = nn.Linear(hidden_size * self.bidirectional, out_size)\n",
        "\n",
        "\n",
        "#     def init_hidden(self, batch_size):\n",
        "#         hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         return hidden, state\n",
        "\n",
        "#     def forward(self, x, hidden):\n",
        "#         x = x.float()\n",
        "#         all_outputs, hidden = self.lstm(x, hidden)\n",
        "#         h_n, _ = hidden\n",
        "#         # h_n = torch.flatten(h_n, 1)\n",
        "#         # print(h_n)\n",
        "#         x = self.fc(h_n)\n",
        "#         return x, hidden\n",
        "\n",
        "# model = LSTMRegressor(1,5,2,5).to(device)\n",
        "# model"
      ],
      "metadata": {
        "id": "ibqd8eiGapul"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module): #Coś nie tak\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, out_size, lin_size, bidirectional = False):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        if bidirectional:\n",
        "            self.bidirectional = 2\n",
        "        else:\n",
        "            self.bidirectional = 1\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "        self.fc = nn.Linear(hidden_size * self.bidirectional, lin_size)\n",
        "        self.fc2 = nn.Linear(lin_size,out_size)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "        return hidden, state\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.float()\n",
        "        all_outputs, hidden = self.lstm(x, hidden)\n",
        "        h_n, _ = hidden\n",
        "        # h_n = torch.flatten(h_n, 1)\n",
        "        # print(h_n)\n",
        "        x = F.relu(self.fc(h_n[-1]))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        # x = F.softmax(x, dim=1)\n",
        "        return x, hidden\n",
        "\n",
        "model = LSTMClassifier(1,5,2,5,5).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR3Gw-nNEsaJ",
        "outputId": "7f874826-40e1-4c0d-c412-1ad9ed707790"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (lstm): LSTM(1, 5, num_layers=2, dropout=0.4)\n",
              "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "loss_fun = nn.CrossEntropyLoss(weight=class_weights.to(device))"
      ],
      "metadata": {
        "id": "bXV1bOsGv3Mm"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = LSTMRegressor(1,5,2,5).to(device)\n",
        "# num_epochs = 1000\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     for x, targets, x_len, target_len in train_loader:\n",
        "#         x = x.to(device).unsqueeze(2)\n",
        "#         targets = targets.to(device)\n",
        "#         hidden, state = model.init_hidden(x.size(0))\n",
        "#         hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "#         x = torch.transpose(x, 0, 1)\n",
        "#         print(x)\n",
        "#         preds, _ = model(x, (hidden, state))\n",
        "#         # print(preds.size())\n",
        "#         # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "\n",
        "#         # preds = preds.squeeze(2)\n",
        "#         # preds = preds.mean(dim=1)\n",
        "#         optimizer.zero_grad()\n",
        "#         # mask = targets != pad\n",
        "#         # print(preds.size())\n",
        "#         loss = loss_fun(preds, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     if epoch % 10 == 0:\n",
        "#         print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")"
      ],
      "metadata": {
        "id": "-61lI-xWv9_W"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMClassifier(1,64,5,5, 50).to(device)\n",
        "num_epochs = 1000\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, targets, x_len, target_len in train_loader:\n",
        "        x = x.to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "        x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "        preds, _ = model(x_packed, (hidden, state))\n",
        "        # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "        # preds = preds.squeeze(2)\n",
        "        # preds = preds.mean(dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fun(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")\n",
        "        true_preds, num_preds = 0., 0.\n",
        "        with torch.no_grad():\n",
        "          for x, targets, x_len, target_len in test_loader:\n",
        "              x = x.to(device).unsqueeze(2)\n",
        "              targets = targets.to(device)\n",
        "              hidden, state = model.init_hidden(x.shape[0])\n",
        "              hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "              x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "              preds_packed, _ = model(x_packed, (hidden, state))\n",
        "              preds, _ = model(x_packed, (hidden, state))\n",
        "              # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "              # preds = preds.squeeze(2)\n",
        "              # preds = preds.mean(dim=1)\n",
        "              preds = torch.argmax(preds, dim=1)\n",
        "              targets = torch.argmax(targets, dim=1)\n",
        "              true_preds += (preds == targets).sum().item()\n",
        "              num_preds += targets.shape[0]\n",
        "        acc = true_preds / num_preds\n",
        "        print(f\"Accuracy: {100.0*acc:4.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_qk-PV4Wk3",
        "outputId": "f540fdd1-1a7f-4883-883e-05943bca86e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.215\n",
            "Accuracy: 54.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_preds, num_preds = 0., 0.\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, targets, x_len, target_len in test_loader:\n",
        "        x = x.to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.shape[0])\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "        x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "        preds_packed, _ = model(x_packed, (hidden, state))\n",
        "        preds, _ = model(x_packed, (hidden, state))\n",
        "        # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "        # preds = preds.squeeze(2)\n",
        "        # preds = preds.mean(dim=1)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        targets = torch.argmax(targets, dim=1)\n",
        "        print(\"preds\",preds)\n",
        "        print(targets)\n",
        "        true_preds += (preds == targets).sum().item()\n",
        "        num_preds += targets.shape[0]\n",
        "acc = true_preds / num_preds\n",
        "print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ],
      "metadata": {
        "id": "Ab5xtCcdtyyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3516cc56-7e8b-4a6e-824d-25c411ec9620"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "preds tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "preds tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4], device='cuda:0')\n",
            "preds tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
            "       device='cuda:0')\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
            "       device='cuda:0')\n",
            "Accuracy of the model: 41.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6LQd7lyEmuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}