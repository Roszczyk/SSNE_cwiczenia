{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ],
      "metadata": {
        "id": "aF7NhtuRuj3m"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQk7qP3Wszsx",
        "outputId": "ff8b059e-9db4-44a5-abba-55d43b60dcbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlL-F0T8ujBn",
        "outputId": "509df563-8104-4f0c-8c26-f8481d9ff25b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/train.pkl', 'rb') as f:\n",
        "    data_set = pickle.load(f)"
      ],
      "metadata": {
        "id": "HRKMm6B2uwFd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dz5vfxHvMmg",
        "outputId": "93ec3e95-11cc-4033-a414-f9dfc7a80f6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "targets = []\n",
        "for i in data_set:\n",
        "  data.append(i[0])\n",
        "  targets.append(i[1])"
      ],
      "metadata": {
        "id": "viCp1NWgEI53"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_array = np.array(targets)\n",
        "targets_array_reshaped = targets_array.reshape(-1, 1)\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "ohe.fit(targets_array_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "x-GkoyTUrksk",
        "outputId": "d760abc2-f79c-46e7-e957-78bdfaebc90f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(handle_unknown='ignore', sparse_output=False)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets_ohe = ohe.transform(targets_array_reshaped)\n",
        "print(targets_ohe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52b_-RYisjYn",
        "outputId": "4e57a36f-b9f3-4bd7-b8b5-1d790d943b53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[-1])\n",
        "print(targets[-1])\n",
        "print(targets_ohe[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6eNqA5MIIcl",
        "outputId": "8f783abf-88d5-4223-df23-d7280606a5b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[112. 112.   0.  64.   0. 112.  34. 144. 146. 112. 112. 179. 112. 112.\n",
            "  73.   3.  12. 112. 146.  60.  45. 112.  73.  88. 117.   8. 159. 145.\n",
            " 159. 112.  13.  80.  92. 159. 190. 112. 185. 190.   8.  73. 125.  77.\n",
            "  12.   0. 117. 190. 185. 112. 125. 125.  45.  41.  92.  28.  92.  12.\n",
            " 159.   0.  47.  12.  12.  88.  88.   0.  12. 112. 159.   8. 159.   0.\n",
            "  12. 112. 125.  23.  47.  41. 159. 112.  45.  12.   8. 145.  47.  33.\n",
            "  33.  33. 125.  73.   8.  13.  92.  88. 159. 120. 124. 185. 185.  77.\n",
            " 125.  88.  13. 120. 120. 112. 112. 112. 159. 172.  12. 117.  47.  88.\n",
            "  88.  36.  92.  33. 125.  88.  88.  88.  12. 112. 159.  88. 125. 119.\n",
            "  47.  88.  92. 159. 185. 112.  77.  33. 125.  65. 125.  33. 125.  88.\n",
            "  13. 120. 124. 112.  12.   0.   0.   0.  12. 112. 159. 145.   5.   0.\n",
            "  88. 112.  12.  47.  88.   0.  12. 112. 159. 145.   5.   0.  12. 112.\n",
            " 125. 125.  28. 159.  28.  38.  12. 117.  47. 114. 153.  88. 159.   8.\n",
            "   8. 112.  77.  38.  12.   0. 117. 190. 185. 112.  77.  33.  41. 145.\n",
            " 145. 106. 124. 112.  74.  41.  92.  88.  12. 112.  74.  41.  13. 117.\n",
            "  92.  80. 125. 185. 190. 114. 112. 112. 159.  88.  13.   8.  12. 117.\n",
            "  12. 125. 124. 106. 159. 112. 112. 112. 112. 112.  12.   0. 117. 190.\n",
            " 185. 112. 112. 112. 112. 112.]\n",
            "4\n",
            "[0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "labels = [label for _, label in data_set]\n",
        "\n",
        "class_counts = Counter(labels)\n",
        "classes = []\n",
        "print(\"Liczba klas:\", len(class_counts))\n",
        "for class_label, count in class_counts.items():\n",
        "    classes.append(count)\n",
        "classes = torch.tensor(classes)\n",
        "print(classes)\n",
        "class_weights = 1.0 / classes\n",
        "class_weights = class_weights / class_weights.sum()\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "6xQID-J53J_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c393ea5-7b74-4772-a255-7a06e55a65d0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liczba klas: 5\n",
            "tensor([1630,  478,  154,  441,  236])\n",
            "tensor([0.0391, 0.1332, 0.4135, 0.1444, 0.2698])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KO0y2QIaXYfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariableLenDataset(Dataset):\n",
        "    def __init__(self, in_data, target):\n",
        "        self.data = [(x, y) for x, y in zip(in_data, target)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        in_data, target = self.data[idx]\n",
        "        return in_data, target"
      ],
      "metadata": {
        "id": "JTnWdmaG49BT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices = np.random.rand(len(data_set))>0.3"
      ],
      "metadata": {
        "id": "5JUCzIp8O5NX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices"
      ],
      "metadata": {
        "id": "Bai0Mly3O9Wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c06cd24-cedb-417d-cbd5-d754479e7639"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, ...,  True,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    min_value = min(data)\n",
        "    max_value = max(data)\n",
        "    if min_value == max_value:\n",
        "        # Jeśli wszystkie wartości są takie same, zwracamy listę jedynek\n",
        "        return [1 for _ in data]\n",
        "    else:\n",
        "        normalized_data = [(x - min_value) / (max_value - min_value) for x in data]\n",
        "        return normalized_data\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "W22hjs8zfqvl"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_normalized = []\n",
        "for seq in data:\n",
        "  data_normalized.append(normalize(seq))\n"
      ],
      "metadata": {
        "id": "3gDvCohVPFh-"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = []\n",
        "targets_train = []\n",
        "data_test = []\n",
        "targets_test = []\n",
        "for i in range(len(data_set)):\n",
        "  if train_indices[i] == True:\n",
        "    data_train.append(data_normalized[i])\n",
        "    targets_train.append(targets_ohe[i])\n",
        "  else:\n",
        "    data_test.append(data_normalized[i])\n",
        "    targets_test.append(targets_ohe[i])"
      ],
      "metadata": {
        "id": "SJVg5qU1fvTR"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YESF61pshZ2X",
        "outputId": "d053572c-10ea-48da-eadd-0ff250a6a208"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4293193717277487, 0.4293193717277487, 0.4293193717277487, 0.005235602094240838, 0.9424083769633508, 0.015706806282722512, 0.8324607329842932, 0.02617801047120419, 0.7696335078534031, 0.031413612565445025, 0.06806282722513089, 0.10471204188481675, 0.24607329842931938, 0.041884816753926704, 0.6544502617801047, 0.9685863874345549, 0.4607329842931937, 0.3612565445026178, 0.9685863874345549, 0.6544502617801047, 0.4816753926701571, 0.6544502617801047, 0.6335078534031413, 0.31413612565445026, 0.06806282722513089, 0.4816753926701571, 0.18848167539267016, 0.24607329842931938, 0.02617801047120419, 0.8324607329842932, 0.41361256544502617, 0.41361256544502617, 0.041884816753926704, 0.6492146596858639, 0.6492146596858639, 0.9685863874345549, 0.5235602094240838, 0.010471204188481676, 0.41361256544502617, 0.38219895287958117, 0.3507853403141361, 0.6178010471204188, 0.06806282722513089, 0.041884816753926704, 0.9685863874345549, 0.5549738219895288, 0.41361256544502617, 0.41361256544502617, 0.6282722513089005, 0.3403141361256545, 0.41361256544502617, 0.41361256544502617, 0.06282722513089005, 0.24607329842931938, 0.06282722513089005, 0.06282722513089005, 0.2356020942408377, 0.06282722513089005, 0.774869109947644, 0.900523560209424, 0.21465968586387435, 0.24607329842931938, 0.6544502617801047, 0.6544502617801047, 0.4607329842931937, 0.24607329842931938, 0.10471204188481675, 0.31413612565445026, 0.03664921465968586, 0.4293193717277487, 0.6073298429319371, 0.031413612565445025, 0.02617801047120419, 0.4816753926701571, 0.6335078534031413, 0.4816753926701571, 0.21465968586387435, 0.005235602094240838, 0.774869109947644, 0.041884816753926704, 0.193717277486911, 0.6492146596858639, 0.02617801047120419, 0.6544502617801047, 0.24607329842931938, 0.10471204188481675, 0.9424083769633508, 0.06282722513089005, 0.21465968586387435, 0.8167539267015707, 0.8167539267015707, 0.041884816753926704, 0.6910994764397905, 0.5235602094240838, 0.06806282722513089, 0.10471204188481675, 0.17277486910994763, 0.4607329842931937, 0.24607329842931938, 0.06282722513089005, 0.041884816753926704, 0.16753926701570682, 0.581151832460733, 0.581151832460733, 0.02617801047120419, 0.02617801047120419, 0.6335078534031413, 0.31413612565445026, 0.23036649214659685, 0.31413612565445026, 0.23036649214659685, 0.06282722513089005, 0.4083769633507853, 0.23036649214659685, 0.9424083769633508, 0.06282722513089005, 0.612565445026178, 0.06282722513089005, 1.0, 1.0, 0.1256544502617801, 0.4083769633507853, 0.4083769633507853, 0.10471204188481675, 0.10471204188481675, 0.24607329842931938, 0.4869109947643979, 0.031413612565445025, 0.5916230366492147, 0.6649214659685864, 0.6649214659685864, 0.6335078534031413, 0.10471204188481675, 0.13612565445026178, 1.0, 0.13089005235602094, 0.2094240837696335, 0.44502617801047123, 0.04712041884816754, 0.7329842931937173, 0.6492146596858639, 0.7329842931937173, 0.02617801047120419, 0.4816753926701571, 0.2356020942408377, 0.31413612565445026, 0.35602094240837695, 0.4816753926701571, 0.02617801047120419, 0.4816753926701571, 0.41361256544502617, 0.41361256544502617, 0.03664921465968586, 0.8272251308900523, 0.8272251308900523, 0.031413612565445025, 0.8010471204188482, 0.387434554973822, 0.24607329842931938, 0.24607329842931938, 0.03664921465968586, 0.4816753926701571, 0.4816753926701571, 0.8010471204188482, 0.6335078534031413, 0.2094240837696335, 0.900523560209424, 0.2094240837696335, 0.3717277486910995, 0.31413612565445026, 0.31413612565445026, 0.6335078534031413, 0.10471204188481675, 0.13612565445026178, 1.0, 0.13089005235602094, 0.7539267015706806, 0.31413612565445026, 0.6335078534031413, 0.18848167539267016, 0.6335078534031413, 0.6335078534031413, 0.24607329842931938, 0.21465968586387435, 0.18324607329842932, 0.4869109947643979, 0.4869109947643979, 0.031413612565445025, 0.8010471204188482, 0.387434554973822, 0.24607329842931938, 0.24607329842931938, 0.4607329842931937, 0.4607329842931937, 0.06282722513089005, 0.9424083769633508, 0.387434554973822, 0.18848167539267016, 0.4816753926701571, 0.8010471204188482, 0.7696335078534031, 0.031413612565445025, 0.06806282722513089, 0.10471204188481675, 0.24607329842931938, 0.041884816753926704, 0.6544502617801047, 0.9685863874345549, 0.4816753926701571, 0.06282722513089005, 0.06282722513089005, 0.24607329842931938, 0.9424083769633508, 0.4607329842931937, 0.8324607329842932, 0.02617801047120419, 0.7696335078534031, 0.06806282722513089, 0.06806282722513089, 0.10471204188481675, 0.24607329842931938, 0.041884816753926704, 0.6544502617801047, 0.9685863874345549, 0.4607329842931937, 0.3612565445026178, 0.9685863874345549, 0.6544502617801047, 0.4816753926701571, 0.6544502617801047, 0.6335078534031413, 0.31413612565445026, 0.06806282722513089, 0.06806282722513089, 0.24607329842931938, 0.24607329842931938, 0.900523560209424, 0.24607329842931938, 0.04712041884816754, 0.7329842931937173, 0.06806282722513089, 0.900523560209424, 0.6073298429319371, 0.8272251308900523, 0.21465968586387435, 0.24607329842931938, 0.6544502617801047, 0.6544502617801047, 0.4607329842931937, 0.9424083769633508, 0.9424083769633508, 0.10471204188481675, 0.10471204188481675, 0.10471204188481675, 0.005235602094240838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_size = len(data_set)\n",
        "\n",
        "# train_size = int(0.8 * dataset_size)\n",
        "# test_size = dataset_size - train_size\n",
        "\n",
        "\n",
        "# train_set = VariableLenDataset(data[:train_indices], targets[:train_indices])\n",
        "# test_set = VariableLenDataset(data[train_indices:], targets[train_indices:])"
      ],
      "metadata": {
        "id": "qGbQf80xGZbU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = VariableLenDataset(data_train, targets_train)\n",
        "test_set = VariableLenDataset(data_test, targets_test)"
      ],
      "metadata": {
        "id": "05cbXnJZMV_M"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set[2])"
      ],
      "metadata": {
        "id": "_gSWK_fJK6EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3671b40b-6ee4-44dd-d53d-66e51bc9a149"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([0.3526315789473684, 0.531578947368421, 0.7842105263157895, 0.7842105263157895, 0.7736842105263158, 0.34210526315789475, 0.7736842105263158, 0.7842105263157895, 0.4368421052631579, 0.005263157894736842, 0.4368421052631579, 0.531578947368421, 0.18421052631578946, 0.7, 0.9526315789473684, 0.3473684210526316, 0.4263157894736842, 0.43157894736842106, 0.6947368421052632, 0.2789473684210526, 0.18421052631578946, 0.2789473684210526, 0.34210526315789475, 0.2789473684210526, 0.021052631578947368, 0.3526315789473684, 0.7789473684210526, 0.11052631578947368, 0.02631578947368421, 0.7, 0.7, 0.531578947368421, 0.5894736842105263, 0.39473684210526316, 0.5842105263157895, 0.32105263157894737, 0.48947368421052634, 0.3473684210526316, 0.531578947368421, 1.0, 0.23684210526315788, 0.04736842105263158, 0.031578947368421054, 0.4052631578947368, 0.16842105263157894, 0.8421052631578947, 0.031578947368421054, 0.6578947368421053, 0.02631578947368421, 0.06842105263157895, 0.2736842105263158, 0.8315789473684211, 0.30526315789473685, 0.16842105263157894, 0.968421052631579, 0.30526315789473685, 0.3473684210526316, 0.48947368421052634, 0.3684210526315789, 0.6578947368421053, 0.6473684210526316, 0.42105263157894735, 0.5842105263157895, 0.4052631578947368, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.04736842105263158, 0.8421052631578947, 0.06842105263157895, 0.8263157894736842, 0.531578947368421, 0.5894736842105263, 0.2789473684210526, 0.6421052631578947, 0.19473684210526315, 0.25263157894736843, 0.22105263157894736, 0.22105263157894736, 0.04736842105263158, 0.9105263157894737, 0.20526315789473684, 0.06842105263157895, 0.41578947368421054, 0.06842105263157895, 0.46842105263157896, 0.25263157894736843, 0.631578947368421, 0.11052631578947368, 0.25263157894736843, 0.06842105263157895, 0.8421052631578947, 0.11052631578947368, 0.11052631578947368, 0.4052631578947368, 0.32105263157894737, 0.5842105263157895, 0.7, 0.9789473684210527, 0.6368421052631579, 0.24210526315789474, 0.5842105263157895, 0.5842105263157895, 0.38421052631578945, 0.6578947368421053, 0.9421052631578948, 0.3894736842105263, 0.04736842105263158, 0.41578947368421054, 0.06842105263157895, 0.22631578947368422, 0.9157894736842105, 0.06842105263157895, 0.7947368421052632, 0.4473684210526316, 0.3526315789473684, 0.8052631578947368, 0.3684210526315789, 0.04736842105263158, 0.22105263157894736, 0.8421052631578947, 0.031578947368421054, 0.41578947368421054, 0.23684210526315788, 0.9526315789473684, 0.37894736842105264, 0.48947368421052634, 0.8052631578947368, 0.03684210526315789, 0.06842105263157895, 0.24210526315789474, 0.48947368421052634, 0.6631578947368421, 0.04736842105263158, 0.8263157894736842, 0.06842105263157895, 0.06842105263157895, 0.7736842105263158, 0.04736842105263158, 0.9789473684210527, 0.005263157894736842, 0.46842105263157896, 0.48947368421052634, 0.4789473684210526, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.41578947368421054, 0.48947368421052634, 0.021052631578947368, 0.24210526315789474, 0.5894736842105263, 0.5894736842105263, 0.37894736842105264, 0.7263157894736842, 0.3684210526315789, 0.06842105263157895, 0.46842105263157896, 0.06842105263157895, 0.05789473684210526, 0.04736842105263158, 0.9105263157894737, 0.8263157894736842, 0.8052631578947368, 0.531578947368421, 0.06842105263157895, 0.06842105263157895, 0.4105263157894737, 0.2789473684210526, 0.8263157894736842, 0.7842105263157895, 0.06842105263157895, 0.06842105263157895, 0.7, 0.03684210526315789, 0.03684210526315789, 0.9789473684210527, 0.25263157894736843, 0.6421052631578947, 0.11052631578947368, 0.4789473684210526, 0.34210526315789475, 0.5894736842105263, 0.48947368421052634, 0.5894736842105263, 0.4105263157894737, 0.4052631578947368, 0.3684210526315789, 0.531578947368421, 0.04736842105263158, 0.4052631578947368, 0.4052631578947368, 0.3684210526315789, 0.7, 0.03684210526315789, 0.03684210526315789, 0.04736842105263158, 0.11052631578947368, 0.3894736842105263, 0.4105263157894737, 0.5894736842105263, 0.03684210526315789, 0.04736842105263158, 0.07368421052631578, 0.06842105263157895, 0.9421052631578948, 0.031578947368421054, 0.46842105263157896, 0.8421052631578947, 0.6421052631578947, 0.3105263157894737, 0.23684210526315788, 0.25263157894736843, 0.531578947368421, 0.8263157894736842, 0.06842105263157895, 0.22105263157894736, 0.46842105263157896, 0.3894736842105263, 0.11052631578947368, 0.6631578947368421, 0.06842105263157895, 0.6578947368421053, 0.031578947368421054, 0.005263157894736842, 0.031578947368421054, 0.25263157894736843, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.04736842105263158, 0.8421052631578947, 0.9789473684210527, 0.6578947368421053, 0.531578947368421, 0.5894736842105263, 0.2789473684210526, 0.6421052631578947, 0.19473684210526315, 0.25263157894736843, 0.22105263157894736, 0.22105263157894736, 0.9526315789473684, 0.04736842105263158, 0.3526315789473684, 0.04736842105263158, 0.7736842105263158, 0.9526315789473684, 0.46842105263157896, 0.25263157894736843, 0.631578947368421, 0.11052631578947368, 0.3894736842105263, 0.9105263157894737, 0.8368421052631579, 0.11052631578947368, 0.11052631578947368, 0.11052631578947368, 0.41578947368421054, 0.3684210526315789, 0.06842105263157895, 0.06842105263157895, 0.25263157894736843, 0.25263157894736843, 0.25263157894736843, 0.1631578947368421, 0.04736842105263158, 0.03684210526315789, 0.24210526315789474, 0.06842105263157895, 0.5894736842105263, 0.5894736842105263, 0.20526315789473684, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.7842105263157895, 0.03684210526315789, 0.03684210526315789, 0.9789473684210527, 0.25263157894736843, 0.6421052631578947, 0.11052631578947368, 0.41578947368421054, 0.34210526315789475, 0.5894736842105263, 0.4105263157894737, 0.5894736842105263, 0.4105263157894737, 0.4052631578947368, 0.7473684210526316, 0.531578947368421, 0.04736842105263158, 0.4052631578947368, 0.4052631578947368, 0.03684210526315789, 0.7, 0.531578947368421, 0.5894736842105263, 0.39473684210526316, 0.5842105263157895, 0.32105263157894737, 0.8421052631578947, 0.42105263157894735, 0.031578947368421054, 1.0, 0.5842105263157895, 0.06842105263157895, 0.031578947368421054, 0.4052631578947368, 0.05789473684210526, 0.48947368421052634, 0.031578947368421054, 0.6578947368421053, 0.02631578947368421, 0.06842105263157895, 0.2736842105263158, 0.8315789473684211, 0.30526315789473685, 0.16842105263157894, 0.968421052631579, 0.30526315789473685, 0.3473684210526316, 0.48947368421052634, 0.3684210526315789, 0.6578947368421053, 0.6473684210526316, 0.42105263157894735, 0.5842105263157895, 0.4052631578947368, 0.05789473684210526, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.04736842105263158, 0.8421052631578947, 0.06842105263157895, 0.8263157894736842, 0.531578947368421, 0.5894736842105263, 0.2789473684210526, 0.6421052631578947, 0.19473684210526315, 0.25263157894736843, 0.22105263157894736, 0.22105263157894736, 0.04736842105263158, 0.9105263157894737, 0.20526315789473684, 0.06842105263157895, 0.41578947368421054, 0.06842105263157895, 0.46842105263157896, 0.25263157894736843, 0.631578947368421, 0.11052631578947368, 0.25263157894736843, 0.06842105263157895, 0.06842105263157895, 0.41578947368421054, 0.3368421052631579, 0.4052631578947368, 0.32105263157894737, 0.5842105263157895, 0.8421052631578947, 0.7, 0.6578947368421053, 0.06842105263157895, 0.08421052631578947, 0.5842105263157895, 0.5842105263157895, 0.38421052631578945, 0.6578947368421053, 0.9421052631578948, 0.3894736842105263, 0.04736842105263158, 0.41578947368421054, 0.06842105263157895, 0.06842105263157895, 0.1631578947368421, 0.06842105263157895, 0.7947368421052632, 0.4473684210526316, 0.3526315789473684, 0.8052631578947368, 0.3684210526315789, 0.04736842105263158, 0.22105263157894736, 0.8421052631578947, 0.031578947368421054, 0.41578947368421054, 0.23684210526315788, 0.9526315789473684, 0.37894736842105264, 0.48947368421052634, 0.8263157894736842, 0.7, 0.42105263157894735, 0.24210526315789474, 0.48947368421052634, 0.6631578947368421, 0.04736842105263158, 0.8263157894736842, 0.06842105263157895, 0.06842105263157895, 0.7736842105263158, 0.06842105263157895, 0.9789473684210527, 0.005263157894736842, 0.48947368421052634, 0.5736842105263158, 0.4789473684210526, 0.25263157894736843, 0.06842105263157895, 0.06842105263157895, 0.6631578947368421, 0.48947368421052634, 0.021052631578947368, 0.24210526315789474, 0.5894736842105263, 0.5894736842105263, 0.37894736842105264, 0.7263157894736842, 0.3684210526315789, 0.06842105263157895, 0.25263157894736843, 0.06842105263157895, 0.05789473684210526, 0.04736842105263158, 0.22105263157894736, 0.8263157894736842, 0.8052631578947368, 0.531578947368421, 0.06842105263157895, 0.06842105263157895, 0.39473684210526316, 0.2789473684210526, 0.8263157894736842, 0.7842105263157895, 0.7842105263157895, 0.06842105263157895, 0.3684210526315789, 0.03684210526315789, 0.03684210526315789, 0.9789473684210527, 0.25263157894736843, 0.6421052631578947, 0.11052631578947368, 0.4789473684210526, 0.34210526315789475, 0.5894736842105263, 0.48947368421052634, 0.5894736842105263, 0.4105263157894737, 0.4052631578947368, 0.7473684210526316, 0.531578947368421, 0.04736842105263158, 0.4052631578947368, 0.4052631578947368, 0.3684210526315789, 0.7, 0.531578947368421, 0.03684210526315789, 0.04736842105263158, 0.46842105263157896, 0.25263157894736843, 0.7473684210526316, 0.4105263157894737, 0.03684210526315789, 0.04736842105263158, 0.8052631578947368, 0.06842105263157895, 0.9421052631578948, 0.031578947368421054, 0.46842105263157896, 0.8421052631578947, 0.6421052631578947, 0.3105263157894737, 0.23684210526315788, 0.25263157894736843, 0.531578947368421, 0.8263157894736842, 0.06842105263157895, 0.22105263157894736, 0.46842105263157896, 0.3894736842105263, 0.11052631578947368, 0.6631578947368421, 0.06842105263157895, 0.6578947368421053, 0.031578947368421054, 0.005263157894736842, 0.031578947368421054, 0.25263157894736843, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.8421052631578947, 0.9789473684210527, 0.6578947368421053, 0.03684210526315789, 0.5894736842105263, 0.2789473684210526, 0.6421052631578947, 0.19473684210526315, 0.25263157894736843, 0.22105263157894736, 0.22105263157894736, 0.9526315789473684, 0.04736842105263158, 0.3526315789473684, 0.04736842105263158, 0.7736842105263158, 0.9526315789473684, 0.46842105263157896, 0.25263157894736843, 0.631578947368421, 0.11052631578947368, 0.3894736842105263, 0.06842105263157895, 0.8368421052631579, 0.11052631578947368, 0.11052631578947368, 0.11052631578947368, 0.41578947368421054, 0.3684210526315789, 0.06842105263157895, 0.06842105263157895, 0.25263157894736843, 0.25263157894736843, 0.25263157894736843, 0.6631578947368421, 0.04736842105263158, 0.03684210526315789, 0.06842105263157895, 0.06842105263157895, 0.5894736842105263, 0.5894736842105263, 0.20526315789473684, 0.06842105263157895, 0.06842105263157895, 0.06842105263157895, 0.4368421052631579, 0.03684210526315789, 0.03684210526315789, 0.9789473684210527, 0.25263157894736843, 0.6421052631578947, 0.46842105263157896, 0.4789473684210526, 0.34210526315789475, 0.5894736842105263, 0.48947368421052634, 0.5894736842105263, 0.4105263157894737, 0.4052631578947368, 0.7473684210526316, 0.531578947368421, 0.04736842105263158, 0.4052631578947368, 0.4052631578947368, 0.3684210526315789, 0.7, 0.03684210526315789, 0.03684210526315789, 0.04736842105263158, 0.25263157894736843, 0.25263157894736843, 0.05789473684210526, 0.05789473684210526, 0.4368421052631579, 0.04736842105263158, 0.7842105263157895, 0.20526315789473684, 0.26842105263157895, 0.26842105263157895, 0.18421052631578946, 0.18421052631578946, 0.18421052631578946, 0.18421052631578946, 0.18421052631578946, 0.18421052631578946, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.7631578947368421, 0.0], array([1., 0., 0., 0., 0.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad = -100\n",
        "\n",
        "# def pad_collate(batch, pad_value=pad):\n",
        "#     xx, yy = zip(*batch)\n",
        "#     xx = [torch.tensor(x) for x in xx]\n",
        "#     x_lens = [len(x) for x in xx]\n",
        "#     yy_pad = torch.tensor(yy)\n",
        "#     y_lens = [1] * len(yy)\n",
        "#     # print(xx)\n",
        "#     # print(yy)\n",
        "\n",
        "#     xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
        "#     # yy_pad = pad_sequence(yy, batch_first=True, padding_value=pad_value)\n",
        "\n",
        "#     return xx_pad, yy_pad, x_lens, y_lens\n",
        "\n",
        "def pad_collate(batch, pad_value=pad):\n",
        "    try:\n",
        "        xx, yy = zip(*batch)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error while unpacking batch: {e}\")\n",
        "        print(f\"Batch content: {batch}\")\n",
        "        raise\n",
        "\n",
        "    xx = [torch.tensor(x) for x in xx]\n",
        "    x_lens = [len(x) for x in xx]\n",
        "    yy_pad = torch.tensor(yy)\n",
        "    y_lens = [1] * len(yy)\n",
        "\n",
        "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=pad_value)\n",
        "\n",
        "    return xx_pad, yy_pad, x_lens, y_lens"
      ],
      "metadata": {
        "id": "gT4DVyAfws-7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=256\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "xBcnjNFqIn6H"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_size = len(data)\n",
        "\n",
        "# train_size = int(0.8 * dataset_size)\n",
        "# test_size = dataset_size - train_size\n",
        "\n",
        "# train_set, test_set = random_split(data, [train_size, test_size])\n",
        "\n",
        "# batch_size=4\n",
        "# trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
        "# testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "nE3dVfPevMd-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set[0]\n"
      ],
      "metadata": {
        "id": "n4cE0wPFIndl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e8872a-2d88-4c80-bcc2-3af33473a4fe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ -1.,  -1.,  -1., ...,  78.,  40., 144.]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_set[0][0]))\n",
        "print(len(train_set[1][0]))"
      ],
      "metadata": {
        "id": "8iAC0VKIw776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce55772-f075-40bd-8d94-e93d87036eef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4756\n",
            "5322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R2Ulv0AxYvE",
        "outputId": "2bf1afc3-c827-4658-f53f-a93a84b09637"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[   0.0000,    0.0000,    0.0000,  ..., -100.0000, -100.0000,\n",
              "          -100.0000],\n",
              "         [   0.0000,    0.0000,    0.0000,  ..., -100.0000, -100.0000,\n",
              "          -100.0000],\n",
              "         [   0.0000,    0.0000,    0.0000,  ..., -100.0000, -100.0000,\n",
              "          -100.0000],\n",
              "         ...,\n",
              "         [   0.4294,    0.2294,    0.2294,  ..., -100.0000, -100.0000,\n",
              "          -100.0000],\n",
              "         [   0.0000,    0.0000,    0.0000,  ..., -100.0000, -100.0000,\n",
              "          -100.0000],\n",
              "         [   0.0000,    0.0000,    0.0000,  ..., -100.0000, -100.0000,\n",
              "          -100.0000]], dtype=torch.float64),\n",
              " tensor([[1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 1.]], dtype=torch.float64),\n",
              " [180,\n",
              "  403,\n",
              "  48,\n",
              "  44,\n",
              "  156,\n",
              "  421,\n",
              "  411,\n",
              "  350,\n",
              "  193,\n",
              "  969,\n",
              "  519,\n",
              "  44,\n",
              "  1272,\n",
              "  108,\n",
              "  28,\n",
              "  87,\n",
              "  1925,\n",
              "  300,\n",
              "  195,\n",
              "  216,\n",
              "  258,\n",
              "  16,\n",
              "  104,\n",
              "  44,\n",
              "  282,\n",
              "  212,\n",
              "  584,\n",
              "  189,\n",
              "  848,\n",
              "  1338,\n",
              "  170,\n",
              "  68,\n",
              "  72,\n",
              "  2958,\n",
              "  128,\n",
              "  1625,\n",
              "  196,\n",
              "  228,\n",
              "  184,\n",
              "  607,\n",
              "  268,\n",
              "  52,\n",
              "  284,\n",
              "  3799,\n",
              "  162,\n",
              "  1260,\n",
              "  492,\n",
              "  480,\n",
              "  47,\n",
              "  60,\n",
              "  195,\n",
              "  292,\n",
              "  652,\n",
              "  237,\n",
              "  436,\n",
              "  36,\n",
              "  270,\n",
              "  525,\n",
              "  592,\n",
              "  1707,\n",
              "  27,\n",
              "  52,\n",
              "  593,\n",
              "  84,\n",
              "  1230,\n",
              "  636,\n",
              "  140,\n",
              "  68,\n",
              "  436,\n",
              "  556,\n",
              "  360,\n",
              "  294,\n",
              "  156,\n",
              "  207,\n",
              "  328,\n",
              "  200,\n",
              "  225,\n",
              "  216,\n",
              "  69,\n",
              "  708,\n",
              "  468,\n",
              "  174,\n",
              "  2502,\n",
              "  232,\n",
              "  60,\n",
              "  100,\n",
              "  572,\n",
              "  350,\n",
              "  292,\n",
              "  104,\n",
              "  56,\n",
              "  436,\n",
              "  279,\n",
              "  164,\n",
              "  2860,\n",
              "  280,\n",
              "  1284,\n",
              "  369,\n",
              "  1429,\n",
              "  336,\n",
              "  588,\n",
              "  1576,\n",
              "  2734,\n",
              "  186,\n",
              "  228,\n",
              "  306,\n",
              "  204,\n",
              "  135,\n",
              "  360,\n",
              "  52,\n",
              "  225,\n",
              "  428,\n",
              "  710,\n",
              "  1277,\n",
              "  340,\n",
              "  244,\n",
              "  385,\n",
              "  10,\n",
              "  102,\n",
              "  408,\n",
              "  749,\n",
              "  195,\n",
              "  696,\n",
              "  320,\n",
              "  669,\n",
              "  412,\n",
              "  485,\n",
              "  304,\n",
              "  379,\n",
              "  204,\n",
              "  44,\n",
              "  2097,\n",
              "  117,\n",
              "  104,\n",
              "  615,\n",
              "  692,\n",
              "  64,\n",
              "  564,\n",
              "  356,\n",
              "  1267,\n",
              "  447,\n",
              "  356,\n",
              "  44,\n",
              "  128,\n",
              "  292,\n",
              "  5322,\n",
              "  342,\n",
              "  249,\n",
              "  309,\n",
              "  1342,\n",
              "  340,\n",
              "  213,\n",
              "  294,\n",
              "  875,\n",
              "  552,\n",
              "  276,\n",
              "  180,\n",
              "  1876,\n",
              "  660,\n",
              "  164,\n",
              "  309,\n",
              "  56,\n",
              "  500,\n",
              "  1584,\n",
              "  124,\n",
              "  480,\n",
              "  582,\n",
              "  440,\n",
              "  584,\n",
              "  2128,\n",
              "  301,\n",
              "  1440,\n",
              "  627,\n",
              "  44,\n",
              "  312,\n",
              "  86,\n",
              "  384,\n",
              "  312,\n",
              "  252,\n",
              "  396,\n",
              "  188,\n",
              "  258,\n",
              "  337,\n",
              "  536,\n",
              "  2008,\n",
              "  224,\n",
              "  288,\n",
              "  657,\n",
              "  344,\n",
              "  108,\n",
              "  504,\n",
              "  192,\n",
              "  1428,\n",
              "  465,\n",
              "  358,\n",
              "  336,\n",
              "  537,\n",
              "  68,\n",
              "  552,\n",
              "  68,\n",
              "  108,\n",
              "  414,\n",
              "  236,\n",
              "  510,\n",
              "  858,\n",
              "  48,\n",
              "  139,\n",
              "  232,\n",
              "  572,\n",
              "  842,\n",
              "  938,\n",
              "  496,\n",
              "  548,\n",
              "  132,\n",
              "  648,\n",
              "  280,\n",
              "  756,\n",
              "  56,\n",
              "  192,\n",
              "  260,\n",
              "  436,\n",
              "  418,\n",
              "  96,\n",
              "  394,\n",
              "  1122,\n",
              "  67,\n",
              "  56,\n",
              "  208,\n",
              "  186,\n",
              "  221,\n",
              "  318,\n",
              "  270,\n",
              "  100,\n",
              "  435,\n",
              "  452,\n",
              "  292,\n",
              "  1604,\n",
              "  166,\n",
              "  222,\n",
              "  96,\n",
              "  268,\n",
              "  428,\n",
              "  864,\n",
              "  342,\n",
              "  614,\n",
              "  220,\n",
              "  252,\n",
              "  354,\n",
              "  1292,\n",
              "  472,\n",
              "  285,\n",
              "  599,\n",
              "  276,\n",
              "  129,\n",
              "  240,\n",
              "  198],\n",
              " [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMRegressor(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, out_size, bidirectional = False):\n",
        "#         super().__init__()\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_size = hidden_size\n",
        "#         if bidirectional:\n",
        "#             self.bidirectional = 2\n",
        "#         else:\n",
        "#             self.bidirectional = 1\n",
        "#         self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "#         self.fc = nn.Linear(320, out_size)\n",
        "#         # self.fc = nn.Linear(hidden_size*90*self.bidirectional, out_size)\n",
        "\n",
        "#     def init_hidden(self, batch_size):\n",
        "#         hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         return hidden, state\n",
        "\n",
        "#     def forward(self, x, hidden):\n",
        "#         # x = torch.transpose(x,0,1)\n",
        "#         x = x.float()\n",
        "#         all_outputs, hidden = self.lstm(x, hidden)\n",
        "#         # all_outputs = torch.transpose(all_outputs,0,1)\n",
        "#         # out = torch.flatten(all_outputs,1)\n",
        "#         # output, _ = pad_packed_sequence(all_outputs, batch_first=True)\n",
        "#         # output = output[:, -1, :]\n",
        "\n",
        "#         h_n, _ = hidden\n",
        "#         h_n = torch.flatten(h_n, 1)  # Flatten the hidden state\n",
        "#         # print(h_n[0])\n",
        "#         x = self.fc(h_n)\n",
        "\n",
        "#         # hidden = torch.flatten(hidden,1)\n",
        "#         # print(hidden[0])\n",
        "#         # x = self.fc(hidden)\n",
        "#         return x, hidden\n",
        "\n",
        "# model = LSTMRegressor(1,5,2,5).to(device)\n",
        "# model"
      ],
      "metadata": {
        "id": "PcFIdl-fB0GW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LSTMRegressor(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, out_size, bidirectional = False):\n",
        "#         super().__init__()\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_size = hidden_size\n",
        "#         if bidirectional:\n",
        "#             self.bidirectional = 2\n",
        "#         else:\n",
        "#             self.bidirectional = 1\n",
        "#         self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "#         self.fc = nn.Linear(hidden_size * self.bidirectional, out_size)\n",
        "\n",
        "\n",
        "#     def init_hidden(self, batch_size):\n",
        "#         hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "#         return hidden, state\n",
        "\n",
        "#     def forward(self, x, hidden):\n",
        "#         x = x.float()\n",
        "#         all_outputs, hidden = self.lstm(x, hidden)\n",
        "#         h_n, _ = hidden\n",
        "#         # h_n = torch.flatten(h_n, 1)\n",
        "#         # print(h_n)\n",
        "#         x = self.fc(h_n)\n",
        "#         return x, hidden\n",
        "\n",
        "# model = LSTMRegressor(1,5,2,5).to(device)\n",
        "# model"
      ],
      "metadata": {
        "id": "ibqd8eiGapul"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module): #Coś nie tak\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, out_size, lin_size, bidirectional = False):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        if bidirectional:\n",
        "            self.bidirectional = 2\n",
        "        else:\n",
        "            self.bidirectional = 1\n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, bidirectional=bidirectional, dropout=0.4)\n",
        "        self.fc = nn.Linear(hidden_size * self.bidirectional, lin_size)\n",
        "        self.fc2 = nn.Linear(lin_size,out_size)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "        state = torch.zeros(self.num_layers*self.bidirectional , batch_size, self.hidden_size)\n",
        "        return hidden, state\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.float()\n",
        "        all_outputs, hidden = self.lstm(x, hidden)\n",
        "        h_n, _ = hidden\n",
        "        # h_n = torch.flatten(h_n, 1)\n",
        "        # print(h_n)\n",
        "        x = F.relu(self.fc(h_n[-1]))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = F.softmax(x, dim=1)\n",
        "        # print(x)\n",
        "        return x, hidden\n",
        "\n",
        "model = LSTMClassifier(1,5,2,5,5).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR3Gw-nNEsaJ",
        "outputId": "34d11841-ce4e-4eba-c7e2-9615615eeb3c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (lstm): LSTM(1, 5, num_layers=2, dropout=0.4)\n",
              "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
              "  (fc2): Linear(in_features=5, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "loss_fun = nn.CrossEntropyLoss(weight=class_weights.to(device))"
      ],
      "metadata": {
        "id": "bXV1bOsGv3Mm"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMClassifier(1,64,5,5, 50).to(device)\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, targets, x_len, target_len in train_loader:\n",
        "        x = x.to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        # print(x)\n",
        "        preds, _ = model(x, (hidden, state))\n",
        "        # print(preds.size())\n",
        "        # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "\n",
        "        # preds = preds.squeeze(2)\n",
        "        # preds = preds.mean(dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        # mask = targets != pad\n",
        "        # print(preds.size())\n",
        "        loss = loss_fun(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")\n",
        "        true_preds, num_preds = 0., 0.\n",
        "        with torch.no_grad():\n",
        "          for x, targets, x_len, target_len in test_loader:\n",
        "              x = x.to(device).unsqueeze(2)\n",
        "              targets = targets.to(device)\n",
        "              hidden, state = model.init_hidden(x.shape[0])\n",
        "              hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "              x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "              preds_packed, _ = model(x_packed, (hidden, state))\n",
        "              preds, _ = model(x_packed, (hidden, state))\n",
        "              # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "              # preds = preds.squeeze(2)\n",
        "              # preds = preds.mean(dim=1)\n",
        "              preds = torch.argmax(preds, dim=1)\n",
        "              targets = torch.argmax(targets, dim=1)\n",
        "              true_preds += (preds == targets).sum().item()\n",
        "              num_preds += targets.shape[0]\n",
        "        acc = true_preds / num_preds\n",
        "        print(f\"Accuracy: {100.0*acc:4.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-61lI-xWv9_W",
        "outputId": "990f3323-a04f-4e52-cd17-259d14c1bab2"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.173\n",
            "Accuracy: 54.78%\n",
            "Epoch: 10, loss: 0.191\n",
            "Accuracy: 54.78%\n",
            "Epoch: 20, loss: 0.153\n",
            "Accuracy: 54.78%\n",
            "Epoch: 30, loss: 0.16\n",
            "Accuracy: 54.78%\n",
            "Epoch: 40, loss: 0.231\n",
            "Accuracy: 54.78%\n",
            "Epoch: 50, loss: 0.16\n",
            "Accuracy: 54.78%\n",
            "Epoch: 60, loss: 0.15\n",
            "Accuracy: 54.78%\n",
            "Epoch: 70, loss: 0.226\n",
            "Accuracy: 54.78%\n",
            "Epoch: 80, loss: 0.199\n",
            "Accuracy: 54.78%\n",
            "Epoch: 90, loss: 0.17\n",
            "Accuracy: 54.78%\n",
            "Epoch: 100, loss: 0.18\n",
            "Accuracy: 54.78%\n",
            "Epoch: 110, loss: 0.134\n",
            "Accuracy: 54.78%\n",
            "Epoch: 120, loss: 0.154\n",
            "Accuracy: 54.78%\n",
            "Epoch: 130, loss: 0.166\n",
            "Accuracy: 54.78%\n",
            "Epoch: 140, loss: 0.13\n",
            "Accuracy: 54.78%\n",
            "Epoch: 150, loss: 0.173\n",
            "Accuracy: 54.78%\n",
            "Epoch: 160, loss: 0.178\n",
            "Accuracy: 54.78%\n",
            "Epoch: 170, loss: 0.161\n",
            "Accuracy: 54.78%\n",
            "Epoch: 180, loss: 0.176\n",
            "Accuracy: 54.78%\n",
            "Epoch: 190, loss: 0.149\n",
            "Accuracy: 54.78%\n",
            "Epoch: 200, loss: 0.174\n",
            "Accuracy: 54.78%\n",
            "Epoch: 210, loss: 0.184\n",
            "Accuracy: 54.78%\n",
            "Epoch: 220, loss: 0.159\n",
            "Accuracy: 54.78%\n",
            "Epoch: 230, loss: 0.15\n",
            "Accuracy: 54.78%\n",
            "Epoch: 240, loss: 0.126\n",
            "Accuracy: 54.78%\n",
            "Epoch: 250, loss: 0.16\n",
            "Accuracy: 54.78%\n",
            "Epoch: 260, loss: 0.159\n",
            "Accuracy: 54.78%\n",
            "Epoch: 270, loss: 0.188\n",
            "Accuracy: 54.78%\n",
            "Epoch: 280, loss: 0.152\n",
            "Accuracy: 54.78%\n",
            "Epoch: 290, loss: 0.162\n",
            "Accuracy: 54.78%\n",
            "Epoch: 300, loss: 0.197\n",
            "Accuracy: 54.78%\n",
            "Epoch: 310, loss: 0.197\n",
            "Accuracy: 54.78%\n",
            "Epoch: 320, loss: 0.152\n",
            "Accuracy: 54.78%\n",
            "Epoch: 330, loss: 0.155\n",
            "Accuracy: 54.78%\n",
            "Epoch: 340, loss: 0.156\n",
            "Accuracy: 54.78%\n",
            "Epoch: 350, loss: 0.157\n",
            "Accuracy: 54.78%\n",
            "Epoch: 360, loss: 0.179\n",
            "Accuracy: 54.78%\n",
            "Epoch: 370, loss: 0.153\n",
            "Accuracy: 54.78%\n",
            "Epoch: 380, loss: 0.14\n",
            "Accuracy: 54.78%\n",
            "Epoch: 390, loss: 0.157\n",
            "Accuracy: 54.78%\n",
            "Epoch: 400, loss: 0.18\n",
            "Accuracy: 54.78%\n",
            "Epoch: 410, loss: 0.163\n",
            "Accuracy: 54.78%\n",
            "Epoch: 420, loss: 0.161\n",
            "Accuracy: 54.78%\n",
            "Epoch: 430, loss: 0.154\n",
            "Accuracy: 54.78%\n",
            "Epoch: 440, loss: 0.158\n",
            "Accuracy: 54.78%\n",
            "Epoch: 450, loss: 0.177\n",
            "Accuracy: 54.78%\n",
            "Epoch: 460, loss: 0.192\n",
            "Accuracy: 54.78%\n",
            "Epoch: 470, loss: 0.183\n",
            "Accuracy: 54.78%\n",
            "Epoch: 480, loss: 0.182\n",
            "Accuracy: 54.78%\n",
            "Epoch: 490, loss: 0.153\n",
            "Accuracy: 54.78%\n",
            "Epoch: 500, loss: 0.16\n",
            "Accuracy: 54.78%\n",
            "Epoch: 510, loss: 0.192\n",
            "Accuracy: 54.78%\n",
            "Epoch: 520, loss: 0.183\n",
            "Accuracy: 54.78%\n",
            "Epoch: 530, loss: 0.165\n",
            "Accuracy: 54.78%\n",
            "Epoch: 540, loss: 0.138\n",
            "Accuracy: 54.78%\n",
            "Epoch: 550, loss: 0.152\n",
            "Accuracy: 54.78%\n",
            "Epoch: 560, loss: 0.133\n",
            "Accuracy: 54.78%\n",
            "Epoch: 570, loss: 0.177\n",
            "Accuracy: 54.78%\n",
            "Epoch: 580, loss: 0.159\n",
            "Accuracy: 54.78%\n",
            "Epoch: 590, loss: 0.172\n",
            "Accuracy: 54.78%\n",
            "Epoch: 600, loss: 0.177\n",
            "Accuracy: 54.78%\n",
            "Epoch: 610, loss: 0.156\n",
            "Accuracy: 54.78%\n",
            "Epoch: 620, loss: 0.121\n",
            "Accuracy: 54.78%\n",
            "Epoch: 630, loss: 0.145\n",
            "Accuracy: 54.78%\n",
            "Epoch: 640, loss: 0.158\n",
            "Accuracy: 54.78%\n",
            "Epoch: 650, loss: 0.173\n",
            "Accuracy: 54.78%\n",
            "Epoch: 660, loss: 0.214\n",
            "Accuracy: 54.78%\n",
            "Epoch: 670, loss: 0.145\n",
            "Accuracy: 54.78%\n",
            "Epoch: 680, loss: 0.162\n",
            "Accuracy: 54.78%\n",
            "Epoch: 690, loss: 0.176\n",
            "Accuracy: 54.78%\n",
            "Epoch: 700, loss: 0.179\n",
            "Accuracy: 54.78%\n",
            "Epoch: 710, loss: 0.186\n",
            "Accuracy: 54.78%\n",
            "Epoch: 720, loss: 0.175\n",
            "Accuracy: 54.78%\n",
            "Epoch: 730, loss: 0.158\n",
            "Accuracy: 54.78%\n",
            "Epoch: 740, loss: 0.165\n",
            "Accuracy: 54.78%\n",
            "Epoch: 750, loss: 0.155\n",
            "Accuracy: 54.78%\n",
            "Epoch: 760, loss: 0.132\n",
            "Accuracy: 54.78%\n",
            "Epoch: 770, loss: 0.186\n",
            "Accuracy: 54.78%\n",
            "Epoch: 780, loss: 0.135\n",
            "Accuracy: 54.78%\n",
            "Epoch: 790, loss: 0.205\n",
            "Accuracy: 54.78%\n",
            "Epoch: 800, loss: 0.193\n",
            "Accuracy: 54.78%\n",
            "Epoch: 810, loss: 0.193\n",
            "Accuracy: 54.78%\n",
            "Epoch: 820, loss: 0.196\n",
            "Accuracy: 54.78%\n",
            "Epoch: 830, loss: 0.167\n",
            "Accuracy: 54.78%\n",
            "Epoch: 840, loss: 0.149\n",
            "Accuracy: 54.78%\n",
            "Epoch: 850, loss: 0.183\n",
            "Accuracy: 54.78%\n",
            "Epoch: 860, loss: 0.205\n",
            "Accuracy: 54.78%\n",
            "Epoch: 870, loss: 0.179\n",
            "Accuracy: 54.78%\n",
            "Epoch: 880, loss: 0.165\n",
            "Accuracy: 54.78%\n",
            "Epoch: 890, loss: 0.189\n",
            "Accuracy: 54.78%\n",
            "Epoch: 900, loss: 0.167\n",
            "Accuracy: 54.78%\n",
            "Epoch: 910, loss: 0.197\n",
            "Accuracy: 54.78%\n",
            "Epoch: 920, loss: 0.159\n",
            "Accuracy: 54.78%\n",
            "Epoch: 930, loss: 0.214\n",
            "Accuracy: 54.78%\n",
            "Epoch: 940, loss: 0.16\n",
            "Accuracy: 54.78%\n",
            "Epoch: 950, loss: 0.173\n",
            "Accuracy: 54.78%\n",
            "Epoch: 960, loss: 0.179\n",
            "Accuracy: 54.78%\n",
            "Epoch: 970, loss: 0.175\n",
            "Accuracy: 54.78%\n",
            "Epoch: 980, loss: 0.153\n",
            "Accuracy: 54.78%\n",
            "Epoch: 990, loss: 0.185\n",
            "Accuracy: 54.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMClassifier(1,64,5,5, 50).to(device)\n",
        "num_epochs = 1000\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for x, targets, x_len, target_len in train_loader:\n",
        "        x = x.to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.size(0))\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "        x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "        preds, _ = model(x_packed, (hidden, state))\n",
        "        # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "        # preds = preds.squeeze(2)\n",
        "        # preds = preds.mean(dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fun(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}, loss: {loss.item():.3}\")\n",
        "        true_preds, num_preds = 0., 0.\n",
        "        with torch.no_grad():\n",
        "          for x, targets, x_len, target_len in test_loader:\n",
        "              x = x.to(device).unsqueeze(2)\n",
        "              targets = targets.to(device)\n",
        "              hidden, state = model.init_hidden(x.shape[0])\n",
        "              hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "              x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "              preds_packed, _ = model(x_packed, (hidden, state))\n",
        "              preds, _ = model(x_packed, (hidden, state))\n",
        "              # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "              # preds = preds.squeeze(2)\n",
        "              # preds = preds.mean(dim=1)\n",
        "              preds = torch.argmax(preds, dim=1)\n",
        "              targets = torch.argmax(targets, dim=1)\n",
        "              true_preds += (preds == targets).sum().item()\n",
        "              num_preds += targets.shape[0]\n",
        "        acc = true_preds / num_preds\n",
        "        print(f\"Accuracy: {100.0*acc:4.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kh_qk-PV4Wk3",
        "outputId": "632fc5a6-979a-4ddf-fe88-4a9e40e714c1"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.196\n",
            "Accuracy: 13.40%\n",
            "Epoch: 10, loss: 0.179\n",
            "Accuracy: 13.40%\n",
            "Epoch: 20, loss: 0.175\n",
            "Accuracy: 13.40%\n",
            "Epoch: 30, loss: 0.184\n",
            "Accuracy: 13.40%\n",
            "Epoch: 40, loss: 0.186\n",
            "Accuracy: 13.40%\n",
            "Epoch: 50, loss: 0.151\n",
            "Accuracy: 13.40%\n",
            "Epoch: 60, loss: 0.175\n",
            "Accuracy: 13.40%\n",
            "Epoch: 70, loss: 0.196\n",
            "Accuracy: 13.40%\n",
            "Epoch: 80, loss: 0.167\n",
            "Accuracy: 13.40%\n",
            "Epoch: 90, loss: 0.226\n",
            "Accuracy: 13.40%\n",
            "Epoch: 100, loss: 0.197\n",
            "Accuracy: 13.40%\n",
            "Epoch: 110, loss: 0.179\n",
            "Accuracy: 13.40%\n",
            "Epoch: 120, loss: 0.163\n",
            "Accuracy: 13.40%\n",
            "Epoch: 130, loss: 0.164\n",
            "Accuracy: 13.40%\n",
            "Epoch: 140, loss: 0.207\n",
            "Accuracy: 13.40%\n",
            "Epoch: 150, loss: 0.148\n",
            "Accuracy: 13.40%\n",
            "Epoch: 160, loss: 0.199\n",
            "Accuracy: 13.40%\n",
            "Epoch: 170, loss: 0.12\n",
            "Accuracy: 13.40%\n",
            "Epoch: 180, loss: 0.152\n",
            "Accuracy: 13.40%\n",
            "Epoch: 190, loss: 0.136\n",
            "Accuracy: 13.40%\n",
            "Epoch: 200, loss: 0.134\n",
            "Accuracy: 13.40%\n",
            "Epoch: 210, loss: 0.232\n",
            "Accuracy: 13.40%\n",
            "Epoch: 220, loss: 0.157\n",
            "Accuracy: 13.40%\n",
            "Epoch: 230, loss: 0.165\n",
            "Accuracy: 13.40%\n",
            "Epoch: 240, loss: 0.136\n",
            "Accuracy: 13.40%\n",
            "Epoch: 250, loss: 0.203\n",
            "Accuracy: 13.40%\n",
            "Epoch: 260, loss: 0.179\n",
            "Accuracy: 13.40%\n",
            "Epoch: 270, loss: 0.181\n",
            "Accuracy: 13.40%\n",
            "Epoch: 280, loss: 0.157\n",
            "Accuracy: 13.40%\n",
            "Epoch: 290, loss: 0.155\n",
            "Accuracy: 13.40%\n",
            "Epoch: 300, loss: 0.137\n",
            "Accuracy: 13.40%\n",
            "Epoch: 310, loss: 0.137\n",
            "Accuracy: 13.40%\n",
            "Epoch: 320, loss: 0.195\n",
            "Accuracy: 13.40%\n",
            "Epoch: 330, loss: 0.181\n",
            "Accuracy: 13.40%\n",
            "Epoch: 340, loss: 0.146\n",
            "Accuracy: 13.40%\n",
            "Epoch: 350, loss: 0.171\n",
            "Accuracy: 13.40%\n",
            "Epoch: 360, loss: 0.194\n",
            "Accuracy: 13.40%\n",
            "Epoch: 370, loss: 0.179\n",
            "Accuracy: 13.40%\n",
            "Epoch: 380, loss: 0.179\n",
            "Accuracy: 13.40%\n",
            "Epoch: 390, loss: 0.163\n",
            "Accuracy: 13.40%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-f3712462e7a4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-76b5b91029d1>\u001b[0m in \u001b[0;36mpad_collate\u001b[0;34m(batch, pad_value)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0myy_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-76b5b91029d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0myy_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_preds, num_preds = 0., 0.\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, targets, x_len, target_len in test_loader:\n",
        "        x = x.to(device).unsqueeze(2)\n",
        "        targets = targets.to(device)\n",
        "        hidden, state = model.init_hidden(x.shape[0])\n",
        "        hidden, state = hidden.to(device), state.to(device)\n",
        "\n",
        "        x_packed = pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n",
        "        preds_packed, _ = model(x_packed, (hidden, state))\n",
        "        preds, _ = model(x_packed, (hidden, state))\n",
        "        # preds = torch.transpose(preds, 0, 1)\n",
        "\n",
        "        # preds = preds.squeeze(2)\n",
        "        # preds = preds.mean(dim=1)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        targets = torch.argmax(targets, dim=1)\n",
        "        print(\"preds\",preds)\n",
        "        print(targets)\n",
        "        true_preds += (preds == targets).sum().item()\n",
        "        num_preds += targets.shape[0]\n",
        "acc = true_preds / num_preds\n",
        "print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ],
      "metadata": {
        "id": "Ab5xtCcdtyyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10f425d-5b8c-4501-ee34-64ef038613f9"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "preds tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "preds tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4], device='cuda:0')\n",
            "preds tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
            "       device='cuda:0')\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
            "       device='cuda:0')\n",
            "Accuracy of the model: 5.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6LQd7lyEmuw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}